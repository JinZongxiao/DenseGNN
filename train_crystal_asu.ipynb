{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 02:22:17.749549: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-03 02:22:17.924131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-03 02:22:18.361579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/anaconda3/envs/kgcnn/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from kgcnn.data.utils import save_pickle_file, load_pickle_file\n",
    "from datetime import timedelta\n",
    "from tensorflow_addons import optimizers\n",
    "from kgcnn.data.transform.scaler.standard import StandardLabelScaler\n",
    "from kgcnn.data.transform.scaler.molecule import QMGraphLabelScaler\n",
    "import kgcnn.training.scheduler\n",
    "from kgcnn.training.history import save_history_score, load_history_list\n",
    "from kgcnn.metrics.metrics import ScaledMeanAbsoluteError, ScaledRootMeanSquaredError\n",
    "from sklearn.model_selection import KFold\n",
    "from kgcnn.training.hyper import HyperParameter\n",
    "from kgcnn.data.serial import deserialize as deserialize_dataset\n",
    "from kgcnn.model.serial import deserialize as deserialize_model\n",
    "from kgcnn.utils.plots import plot_train_test_loss, plot_predict_true\n",
    "from kgcnn.utils.devices import set_devices_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of argparse: {'hyper': 'training/hyper/hyper_mp_e_form.py', 'category': None, 'model': 'DenseGNN', 'dataset': 'MatProjectEFormDataset', 'make': 'make_model_asu', 'gpu': None, 'seed': 42, 'fold': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hyper': 'training/hyper/hyper_mp_e_form.py',\n",
       " 'category': None,\n",
       " 'model': 'DenseGNN',\n",
       " 'dataset': 'MatProjectEFormDataset',\n",
       " 'make': 'make_model_asu',\n",
       " 'gpu': None,\n",
       " 'seed': 42,\n",
       " 'fold': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GNNTrainConfig:\n",
    "    def __init__(self, \n",
    "                 hyper=\"hyper/hyper_mp_e_form.py\", \n",
    "                 category= None, \n",
    "                 model=None, \n",
    "                 dataset=None, \n",
    "                 make= None, \n",
    "                 gpu=None,\n",
    "                 fold=None,\n",
    "                 seed=42):\n",
    "        self.hyper = hyper\n",
    "        self.category = category\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.make = make\n",
    "        self.gpu = gpu \n",
    "        self.seed = seed\n",
    "        self.fold = fold\n",
    "\n",
    "    def to_dict(self):\n",
    "        return vars(self)\n",
    "\n",
    "# Usage:\n",
    "config = GNNTrainConfig(hyper=\"training/hyper/hyper_mp_e_form.py\", model='DenseGNN', make='make_model_asu', dataset='MatProjectEFormDataset', seed=42)\n",
    "print(\"Input of argparse:\", config.to_dict())\n",
    "args = config.to_dict()\n",
    "args\n",
    "\n",
    "\n",
    "# python training/train_crystal.py --dataset MatProjectGapDataset --model GATv2 --category GATv2 --hyper training/hyper/hyper_mp_gap.py\n",
    "# python training/train_crystal.py --dataset MatProjectEFormDataset --model GIN --category GIN --hyper training/hyper/hyper_mp_e_form.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "np.random.seed(args[\"seed\"])\n",
    "tf.random.set_seed(args[\"seed\"])\n",
    "tf.keras.utils.set_random_seed(args[\"seed\"])\n",
    "\n",
    "# Assigning GPU.\n",
    "set_devices_gpu(args[\"gpu\"])\n",
    "\n",
    "# A class `HyperParameter` is used to expose and verify hyperparameter.\n",
    "# The hyperparameter is stored as a dictionary with sectiomegnet 'model', 'dataset' and 'training'.\n",
    "hyper = HyperParameter(\n",
    "    hyper_info=args[\"hyper\"], hyper_category=args[\"category\"],\n",
    "    model_name=args[\"model\"], model_class=args[\"make\"], dataset_class=args[\"dataset\"])\n",
    "hyper.verify()\n",
    "\n",
    "# Loading a specific per-defined dataset from a module in kgcnn.data.datasets.\n",
    "# Those sub-classed classes are named after the dataset like e.g. `MatProjectEFormDataset`\n",
    "# If no name is given, a general `CrystalDataset` is constructed.\n",
    "# However, the construction then must be fully defined in the data section of the hyperparameter,\n",
    "# including all methods to run on the dataset. Information required in hyperparameter are for example 'file_path',\n",
    "# 'data_directory' etc.\n",
    "# Making a custom training script rather than configuring the dataset via hyperparameter can be\n",
    "# more convenient.\n",
    "dataset = deserialize_dataset(hyper[\"dataset\"])\n",
    "\n",
    "# Check if dataset has the required properties for model input. This includes a quick shape comparison.\n",
    "# The name of the keras `Input` layer of the model is directly connected to property of the dataset.\n",
    "# Example 'edge_indices' or 'node_attributes'. This couples the keras model to the dataset.\n",
    "dataset.assert_valid_model_input(hyper[\"model\"][\"config\"][\"inputs\"])\n",
    "\n",
    "# Filter the dataset for invalid graphs. At the moment invalid graphs are graphs which do not have the property set,\n",
    "# which is required by the model's input layers, or if a tensor-like property has zero length.\n",
    "dataset.clean(hyper[\"model\"][\"config\"][\"inputs\"])\n",
    "data_length = len(dataset)  # Length of the cleaned dataset.\n",
    "\n",
    "# Train on graph labels. Must be defined by the dataset.\n",
    "labels = np.array(dataset.obtain_property(\"graph_labels\"))\n",
    "label_names = dataset.label_names\n",
    "label_units = dataset.label_units\n",
    "if len(labels.shape) <= 1:\n",
    "    labels = np.expand_dims(labels, axis=-1)\n",
    "\n",
    "# Training on multiple targets for regression.\n",
    "multi_target_indices = hyper[\"training\"][\"multi_target_indices\"] if \"multi_target_indices\" in hyper[\n",
    "    \"training\"] else None\n",
    "if multi_target_indices is not None:\n",
    "    labels = labels[:, multi_target_indices]\n",
    "    if label_names is not None:\n",
    "        label_names = [label_names[i] for i in multi_target_indices]\n",
    "    if label_units is not None:\n",
    "        label_units = [label_units[i] for i in multi_target_indices]\n",
    "print(\"Labels '%s' in '%s' have shape '%s'.\" % (label_names, label_units, labels.shape))\n",
    "\n",
    "# Make output directory\n",
    "filepath = hyper.results_file_path()\n",
    "postfix_file = hyper[\"info\"][\"postfix_file\"]\n",
    "\n",
    "# For Crystals, also the atomic number is required to properly pre-scale extensive quantities like total energy.\n",
    "atoms = dataset.obtain_property(\"node_number\")\n",
    "\n",
    "# Cross-validation via random KFold split form `sklearn.model_selection`.\n",
    "kf = KFold(**hyper[\"training\"][\"cross_validation\"][\"config\"])\n",
    "\n",
    "# Training on splits. Since training on crystal datasets can be expensive, there is a 'execute_splits' parameter to not\n",
    "# train on all splits for testing.\n",
    "execute_folds = args[\"fold\"]\n",
    "if \"execute_folds\" in hyper[\"training\"]:\n",
    "    execute_folds = hyper[\"training\"][\"execute_folds\"]\n",
    "model, hist, x_test, y_test, scaler, atoms_test = None, None, None, None, None, None\n",
    "train_test_indices = [\n",
    "    (train_index, test_index) for train_index, test_index in kf.split(X=np.zeros((data_length, 1)), y=labels)]\n",
    "\n",
    "num_folds = len(train_test_indices)\n",
    "splits_done = 0\n",
    "time_list = []\n",
    "train_indices_all, test_indices_all = [], []\n",
    "for current_fold, (train_index, test_index) in enumerate(train_test_indices):\n",
    "    test_indices_all.append(test_index)\n",
    "    train_indices_all.append(train_index)\n",
    "    print(train_indices_all[0].shape)\n",
    "\n",
    "    # Only do execute_splits out of the k-folds of cross-validation.\n",
    "    if execute_folds:\n",
    "        if current_fold not in execute_folds:\n",
    "            continue\n",
    "    print(\"Running training on fold: %s\" % current_fold)\n",
    "\n",
    "    # Make the model for current split using model kwargs from hyperparameter.\n",
    "    # They are always updated on top of the models default kwargs.\n",
    "    model = deserialize_model(hyper[\"model\"])\n",
    "\n",
    "    # First select training and test graphs from indices, then convert them into tensorflow tensor\n",
    "    # representation. Which property of the dataset and whether the tensor will be ragged is retrieved from the\n",
    "    # kwargs of the keras `Input` layers ('name' and 'ragged').\n",
    "    x_train, y_train = dataset[train_index].tensor(hyper[\"model\"][\"config\"][\"inputs\"]), labels[train_index]\n",
    "    x_test, y_test = dataset[test_index].tensor(hyper[\"model\"][\"config\"][\"inputs\"]), labels[test_index]\n",
    "    # Also keep the same information for atomic numbers of the structures.\n",
    "    atoms_test = [atoms[i] for i in test_index]\n",
    "    atoms_train = [atoms[i] for i in train_index]\n",
    "\n",
    "    # Normalize training and test targets via a sklearn `StandardScaler`. No other scaler are used at the moment.\n",
    "    # Scaler is applied to target if 'scaler' appears in hyperparameter. Only use for regression.\n",
    "    if \"scaler\" in hyper[\"training\"]:\n",
    "        print(\"Using StandardScaler.\")\n",
    "        if hyper[\"training\"][\"scaler\"][\"class_name\"] == \"QMGraphLabelScaler\":\n",
    "            scaler = QMGraphLabelScaler(**hyper[\"training\"][\"scaler\"][\"config\"])\n",
    "        else:\n",
    "            scaler = StandardLabelScaler(**hyper[\"training\"][\"scaler\"][\"config\"])\n",
    "\n",
    "        y_train = scaler.fit_transform(y=y_train, atomic_number=atoms_train)\n",
    "        y_test = scaler.transform(y=y_test, atomic_number=atoms_test)\n",
    "        scaler_scale = scaler.get_scaling()\n",
    "\n",
    "        # If scaler was used we add rescaled standard metrics to compile, since otherwise the keras history will not\n",
    "        # directly log the original target values, but the scaled ones.\n",
    "        mae_metric = ScaledMeanAbsoluteError(scaler_scale.shape, name=\"scaled_mean_absolute_error\")\n",
    "        rms_metric = ScaledRootMeanSquaredError(scaler_scale.shape, name=\"scaled_root_mean_squared_error\")\n",
    "        if scaler_scale is not None:\n",
    "            mae_metric.set_scale(scaler_scale)\n",
    "            rms_metric.set_scale(scaler_scale)\n",
    "        metrics = [mae_metric, rms_metric]\n",
    "\n",
    "        # Save scaler to file\n",
    "        scaler.save(os.path.join(filepath, f\"scaler{postfix_file}_fold_{current_fold}\"))\n",
    "\n",
    "    else:\n",
    "        print(\"TRAINING: Not using StandardScaler for regression.\")\n",
    "        metrics = None\n",
    "\n",
    "    # Compile model with optimizer and loss\n",
    "    model.compile(**hyper.compile(loss=\"mean_absolute_error\", metrics=metrics))\n",
    "    print(model.summary())\n",
    "\n",
    "    # Start and time training\n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, y_train,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     **hyper.fit())\n",
    "    stop = time.time()\n",
    "    print(\"Print Time for training: \", str(timedelta(seconds=stop - start)))\n",
    "    time_list.append(str(timedelta(seconds=stop - start)))\n",
    "    # Get loss from history\n",
    "    save_pickle_file(hist.history, os.path.join(filepath, f\"history{postfix_file}_fold_{current_fold}.pickle\"))\n",
    "\n",
    "    # Plot prediction\n",
    "    predicted_y = model.predict(x_test)\n",
    "    true_y = y_test\n",
    "\n",
    "    if scaler:\n",
    "        predicted_y = scaler.inverse_transform(y=predicted_y, atomic_number=atoms_test)\n",
    "        true_y = scaler.inverse_transform(y=true_y, atomic_number=atoms_test)\n",
    "\n",
    "    plot_predict_true(predicted_y, true_y,\n",
    "                      filepath=filepath, data_unit=label_units,\n",
    "                      model_name=hyper.model_name, dataset_name=hyper.dataset_class, target_names=label_names,\n",
    "                      file_name=f\"predict{postfix_file}_fold_{current_fold}.png\", show_fig=False)\n",
    "\n",
    "    # Save keras-model to output-folder.\n",
    "    model.save(os.path.join(filepath, f\"model{postfix_file}_fold_{current_fold}\"))\n",
    "\n",
    "    splits_done = splits_done + 1\n",
    "\n",
    "history_list = load_history_list(os.path.join(filepath, f\"history{postfix_file}_fold_(i).pickle\"), num_folds)\n",
    "\n",
    "# Plot training- and test-loss vs epochs for all splits.\n",
    "data_unit = hyper[\"data\"][\"data_unit\"] if \"data_unit\" in hyper[\"data\"] else \"\"\n",
    "plot_train_test_loss(history_list, loss_name=None, val_loss_name=None,\n",
    "                     model_name=hyper.model_name, data_unit=data_unit, dataset_name=hyper.dataset_class,\n",
    "                     filepath=filepath, file_name=f\"loss{postfix_file}.png\")\n",
    "\n",
    "# Save original data indices of the splits.\n",
    "np.savez(os.path.join(filepath, f\"{hyper.model_name}_test_indices_{postfix_file}.npz\"), *test_indices_all)\n",
    "np.savez(os.path.join(filepath, f\"{hyper.model_name}_train_indices_{postfix_file}.npz\"), *train_indices_all)\n",
    "\n",
    "# Save hyperparameter again, which were used for this fit.\n",
    "hyper.save(os.path.join(filepath, f\"{hyper.model_name}_hyper{postfix_file}.json\"))\n",
    "\n",
    "# Save score of fit result for as text file.\n",
    "save_history_score(history_list, loss_name=None, val_loss_name=None,\n",
    "                   model_name=hyper.model_name, data_unit=data_unit, dataset_name=hyper.dataset_class,\n",
    "                   model_class=hyper.model_class, multi_target_indices=multi_target_indices,\n",
    "                   execute_folds=execute_folds,seed=args[\"seed\"],\n",
    "                   filepath=filepath, file_name=f\"score{postfix_file}.yaml\", time_list=time_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper_mp_jdft2d\n",
    "# 39.43 cgcnn\n",
    "# 39.53, gogn  44-->43.1--->42.3(GIN+NMPN)  45(GIN+AttentiveFP)\n",
    "# 44 GIN   \n",
    "# 47.68 schnet\n",
    "# 50.57 megnet\n",
    "# 59.2 Nmpn\n",
    "# 60.31 MEGAN\n",
    "\n",
    "# GIN  Schnet  GATv2  GraphSAGE  INorp  \n",
    "\n",
    "\n",
    "# e_form \n",
    "# 0.0255 cgcnn\n",
    "# 0.0272 Schnet\n",
    "# 0.027 megnet\n",
    "# 0.0328 PAiNN\n",
    "# 0.034 GraphSAGE\n",
    "# 0.035 INorp\n",
    "# 0.04 MEGAN\n",
    "# 0.0459 HamNet\n",
    "# 0.0527 GATv2\n",
    "# 0.064(GIN+AttentiveFP)\n",
    "# 0.069 GIN\n",
    "# 0.1483 AttentiveFP\n",
    "# 1.47 Unet\n",
    "\n",
    "\n",
    "# gap\n",
    "# 0.1797 cgcnn\n",
    "# 0.20 GraphSAGE\n",
    "# 0.21 megnet\n",
    "# 0.22 INorp\n",
    "# 0.2317 MEGAN\n",
    "# 0.243 GATv2\n",
    "# 0.2487 Schnet\n",
    "# 0.275(GIN+NMPN)   0.288(GIN+AttentiveFP)\n",
    "# 0.32 GIN\n",
    "# 0.64 AttentiveFP\n",
    "\n",
    "\n",
    "# MatProjectPerovskitesDataset\n",
    "# 0.031 coGN\n",
    "# 0.0354  GATv2\n",
    "# 0.038 Schnet\n",
    "# 0.039 megnet\n",
    "# 0.0396 HamNet\n",
    "# 0.0405 NMPN \n",
    "# 0.0408 GraphSAGE\n",
    "# 0.0418 AttentiveFP\n",
    "# 0.0428 MEGAN    0.044(GIN+NMPN)  0.048 GIN+AttentiveFP\n",
    "# 0.0435 INorp\n",
    "# 0.0508 PAiNN\n",
    "# 0.0539 GIN\n",
    "# 0.0536 MoGAT\n",
    "#   0.08 MAT\n",
    "\n",
    "\n",
    "\n",
    "# hyper_mp_dielectric\n",
    "# 0.303 GIN  0.309  0.3000  0.297  0.30(GIN+NMPN)  0.297(GIN+AttentiveFP)\n",
    "# 0.307 Schnet      0.321(Hamnet+Schnet)---0.308\n",
    "# 0.327 DimeNetPP\n",
    "# 0.324 megnet\n",
    "# 0.3247 MEGAN\n",
    "# 0.3341  NMPN\n",
    "# 0.3468 GraphSAGE\n",
    "# 0.351 GATv2\n",
    "# 0.362  PAiNN\n",
    "# 0.41 AttentiveFP\n",
    "# 0.4563 INorp\n",
    "# 0.50 HamNet\n",
    "# 0.4597 MoGAT\n",
    "\n",
    "\n",
    "\n",
    "# hyper_mp_phonons\n",
    "# 29.19 Megnet\n",
    "# 36.68  HamNet  36.97(GIN+NMPN)\n",
    "# 37 NMPN\n",
    "# 37.25  AttentiveFP  GIN+AttentiveFP----》40  40.54  36.5  37.08  37\n",
    "# 40.8 Schnet           50(Hamnet+Schnet)---44.45 ---- 42  ---- 42.3 \n",
    "# 45.53  GATv2\n",
    "# 49.45 MEGAN\n",
    "# 50.62 GIN\n",
    "# 51.59 PAiNN\n",
    "# 52.51 MoGAT\n",
    "# 84 GraphSAGE\n",
    "# 94 INorp\n",
    "\n",
    "\n",
    "# NMPN(3), HamNet(4), GIN(3), Schnet,     AttentiveFP(1), GATv2(1)，MEGAN(1)  \n",
    "\n",
    "\n",
    "\n",
    "# MatProjectLogKVRHDataset\n",
    "# 0.057 GIN+AttentiveFP  0.0588   0.06 (GIN+NMPN)\n",
    "# 0.0583 GIN     0.058(GIN+GATv2)  \n",
    "# 0.0588 PAiNN\n",
    "# 0.0606 Schnet\n",
    "# 0.068 HamNet\n",
    "# 0.067 NMPN\n",
    "# 0.0687 INorp(charge)  0.0699(no charge) 0.0626  0.0625  0.0598  0.0591  0.0578 \n",
    "# 0.0668 megnet\n",
    "# 0.069 MEGAN\n",
    "# 0.0693 GraphSAGE\n",
    "# 0.0734 GATv2 0.0754(delete n_in)\n",
    "# 0.1044 AttentiveFP\n",
    "\n",
    "\n",
    "\n",
    "# MatProjectLogGVRHDataset\n",
    "# 0.0804 (GIN+NMPN)   \n",
    "# 0.081 GIN+AttentiveFP  ---> 0.079 \n",
    "# 0.0812 GIN\n",
    "# 0.0816 Schnet\n",
    "# 0.0852 HamNet\n",
    "# 0.0851 megnet\n",
    "# 0.0868 INorp\n",
    "# 0.0919 MEGAN\n",
    "# 0.0937 GraphSAGE\n",
    "# 0.096 GATv2\n",
    "# 0.0944 NMPN\n",
    "# 0.1329 AttentiveFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
